### model
model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
adapter_name_or_path: wekafs/jinzeli2/LLaMA-Factory/saves/llama3.1-8b/sft/llama31_8b_sft_bs32_rank8/checkpoint-500
trust_remote_code: true
replace_lm_head: true  # Replace lm_head with AcceptHead (2-layer MLP)

### method
stage: sft
do_train: false  # 不训练，只评估
do_eval: true  # 启用评估
finetuning_type: lora

### dataset
eval_dataset: accept_head
template: llama3
cutoff_len: 4096
max_samples: 2000  # 可以设置最大样本数，或者注释掉使用全部数据
overwrite_cache: false
preprocessing_num_workers: 16
dataloader_num_workers: 4
use_accept_head_format: true  # Enable AcceptHead dataset format

### output
output_dir: saves/llama3.1-8b/lora/eval-checkpoint-500
logging_steps: 10
report_to: wandb  # choices: [none, wandb, tensorboard, swanlab, mlflow]
run_name: default

### eval
per_device_eval_batch_size: 64
compute_accuracy: true  # 启用accuracy计算
accept_head_accuracy_tolerance: 0.2  # Accuracy容差阈值（0.1表示10%的相对误差）

